# -*- coding: utf-8 -*-
"""Ahmed Malek Essid dialect v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19-oQZ3UEt6k9FHWPRQXpp8Y_DrtDgIEQ
"""

!pip install datasets

from transformers import AutoTokenizer, AutoModel
import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset, Subset
from tqdm import tqdm
import pandas as pd
from datasets import load_dataset

from google.colab import drive
drive.mount('/content/drive')



hugging_face_access_token = ""
checkpoint_folder = '/content/drive/MyDrive/checkpoints'

from transformers import AutoTokenizer, AutoModel
import torch

language_model = "CAMeL-Lab/bert-base-arabic-camelbert-msa"
tokenizer = AutoTokenizer.from_pretrained(language_model)

from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("CAMeL-Lab/bert-base-arabic-camelbert-msa")
model = AutoModelForMaskedLM.from_pretrained("CAMeL-Lab/bert-base-arabic-camelbert-msa")

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Load the model
model = AutoModel.from_pretrained(language_model).to(device)

dataset = load_dataset('khaled123/Tunisian_Dialectic_English_Derja')

class TransformerClassifier(nn.Module):
    def __init__(self, model_name, n_classes):
        super(TransformerClassifier, self).__init__()
        self.transformer = AutoModel.from_pretrained(model_name)
        layer_size = self.transformer.config.hidden_size

        self.classifer = nn.Sequential(
            nn.Linear(layer_size, n_classes),
            nn.Softmax(dim=1)
        )


    def forward(self, x, attention_mask):
        with torch.no_grad():
            x = self.transformer(input_ids=x, attention_mask=attention_mask)
        x = x.last_hidden_state[:, 0, :]
        x = self.classifer(x)
        return x




model = TransformerClassifier(language_model, 2).to(device)
print(model)

dataset

'''batch_size = 32

train_loader = DataLoader(dataset["train"], batch_size=batch_size, shuffle=True)
validation_loader = DataLoader(dataset["validation"], batch_size=batch_size, shuffle=False)
test_loader = DataLoader(dataset["test"], batch_size=batch_size, shuffle=False)

for i in train_loader:
    print(i['text'][0])
    break'''

from torch.utils.data import DataLoader, random_split
import torch

print("Available splits:", dataset.keys())

train_size = int(0.8 * len(dataset["train"]))  # Use 80% for training
val_size = len(dataset["train"]) - train_size   # Use 20% for validation



train_dataset, val_dataset = random_split(
    dataset["train"],
    [train_size, val_size],
    generator=torch.Generator().manual_seed(42)
)

# Create DataLoaders
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
validation_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

for batch in train_loader:
    print("Sample text from batch:", batch['text'][0])
    break

print("Dataset columns:", dataset["train"].column_names)
print("\nSample data:")
print(dataset["train"][0])



!pip install emoji regex

import re
import emoji
from datasets import load_dataset

def contains_latin(text):
    """Check if text contains any Latin characters."""
    return bool(re.search('[a-zA-Z]', text))

def clean_text(text):
    """Remove emojis, URLs, and extra whitespace."""
    text = emoji.replace_emoji(text, '')
    text = re.sub(r'http\S+|www.\S+', '', text)
    # Remove extra whitespace
    text = ' '.join(text.split())
    return text.strip()

def filter_and_clean_dataset(example):
    cleaned_text = clean_text(example['text'])


    if not contains_latin(cleaned_text) and cleaned_text:
        example['text'] = cleaned_text
        return True
    return False

dataset = load_dataset('khaled123/Tunisian_Dialectic_English_Derja')

cleaned_dataset = dataset.filter(filter_and_clean_dataset)

print("\nOriginal dataset size:", len(dataset['train']))
print("Cleaned dataset size:", len(cleaned_dataset['train']))


print("\nSample of cleaned data:")
for i in range(3):
    print(f"\nExample {i+1}:")
    print("Text:", cleaned_dataset['train'][i]['text'])

dataset = dataset.filter(filter_and_clean_dataset)

print("First 5 samples from the dataset:")
for i in range(5):
    print(f"Sample {i + 1}:", dataset["train"][i])

print("\nDataset size:", len(dataset["train"]))

from google.colab import drive
import torch
import torch.nn as nn
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.cuda.amp import autocast, GradScaler

drive.mount('/content/drive')

start_epoch = 0
max_epochs = 20
batch_size = 128
max_length = 64
save_snapshots = True
checkpoint_folder = '/content/drive/MyDrive/checkpoints'

# Create checkpoint folder in Google Drive
import os
if not os.path.exists(checkpoint_folder):
    os.makedirs(checkpoint_folder)

# Check for GPU and enable optimizations
device = "cuda" if torch.cuda.is_available() else "cpu"
torch.backends.cudnn.benchmark = True
print(f"Using device: {device}")

# Use only 1% of the data for quick testing
train_size = len(dataset["train"]) // 100
subset_indices = range(train_size)
subset_dataset = dataset["train"].select(subset_indices)
print(f"Training on {train_size} samples (1% of full dataset)")

# Initialize tokenizer
model_name = "CAMeL-Lab/bert-base-arabic-camelbert-mix"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Add padding token
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    print("Added [PAD] token to tokenizer")

class TunisianTextDataset(Dataset):
    def __init__(self, data, tokenizer, max_length):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data[idx]['text']
        encoding = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze()
        }

# Create dataset and dataloader
train_dataset = TunisianTextDataset(subset_dataset, tokenizer, max_length)
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=2,
    pin_memory=True
)

# Initialize model and training components
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
model.resize_token_embeddings(len(tokenizer))
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
scaler = GradScaler()

# Training loop
for epoch in range(start_epoch + 1, max_epochs + 1):
    model.train()
    total_loss = 0
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch}")

    for batch in progress_bar:
        # Move batch to device
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)

        # Mixed precision forward pass
        with autocast():
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=input_ids
            )
            loss = outputs.loss

        # Mixed precision backward pass
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()

        total_loss += loss.item()
        progress_bar.set_postfix({'loss': loss.item()})

    # End of epoch
    avg_loss = total_loss / len(train_loader)
    print(f"\nEpoch {epoch} - Average loss: {avg_loss:.4f}")

    # Save checkpoint
    if save_snapshots:
        checkpoint_path = f"{checkpoint_folder}/epoch-{epoch}.pth"
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': avg_loss,
        }, checkpoint_path)
        print(f"Saved checkpoint to {checkpoint_path}")

def generate_text(prompt, max_length=50):
    model.eval()
    with torch.no_grad():
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            no_repeat_ngram_size=2
        )
        return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Test the model
test_prompts = [
    "احلامي",
    "He made my",
    "عاودولي الصيف"
]

print("\nTesting the model:")
for prompt in test_prompts:
    generated = generate_text(prompt)
    print(f"\nPrompt: {prompt}")
    print(f"Generated: {generated}")

def generate_text(prompt, max_length=50):
    model.eval()
    with torch.no_grad():
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            no_repeat_ngram_size=2
        )
        return tokenizer.decode(outputs[0], skip_special_tokens=True)


test_prompts = [
    "احلامي",
    "He made my",
    "عاودولي الصيف"
]

print("\nTesting the model:")
for prompt in test_prompts:
    generated = generate_text(prompt)
    print(f"\nPrompt: {prompt}")
    print(f"Generated: {generated}")



def test(epoch):
    # Load the checkpoint correctly
    checkpoint = torch.load(f"{checkpoint_folder}/epoch-{epoch}.pth")
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    test_prompts = [
        "احلامي",
        "He made my",
        "عاودولي الصيف"
    ]

    print(f"\nTesting model from epoch {epoch}:")
    for prompt in test_prompts:
        generated = generate_text(prompt)
        print(f"\nPrompt: {prompt}")
        print(f"Generated: {generated}")

test(1)  # Test with checkpoint from epoch 1

!pip install streamlit
import streamlit as st

import streamlit as st

# Set page configuration
st.set_page_config(page_title="Dialect Inspector", page_icon="🔍", layout="wide")

# Add a vibrant header
st.markdown(
    """
    <style>
    .main-header {
        font-size: 36px;
        font-weight: bold;
        color: #ffffff;
        background-color: #4CAF50;
        text-align: center;
        padding: 10px;
        border-radius: 10px;
    }
    </style>
    <div class="main-header">Dialect Inspector</div>
    """,
    unsafe_allow_html=True,
)

# Input section
st.markdown("#### Paste your text below to analyze its dialect or authenticity:")

text = st.text_area("Enter your text here:", height=150)

# Add a button with a custom color
analyze_button = st.markdown(
    """
    <style>
    .stButton button {
        background-color: #6200EA;
        color: white;
        font-size: 16px;
        padding: 10px 20px;
        border-radius: 5px;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# Handle button click
if st.button("Inspect Dialect"):
    if text.strip():
        # Placeholder for AI detection logic
        st.markdown(
            f"""
            <div style="background-color: #FFC107; padding: 10px; border-radius: 5px;">
                <strong>Result:</strong> This text appears to be <em>authentic</em> or <em>synthetic</em>
                (depending on your detection logic).
            </div>
            """,
            unsafe_allow_html=True,
        )
    else:
        st.error("No input detected! Please enter some text to analyze.")

torch.cuda.empty_cache()

